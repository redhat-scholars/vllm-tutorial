<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Advanced Features &amp; Optimization :: vLLM Tutorial</title>
    <link rel="canonical" href="https://redhat-scholars.github.io/vllm-tutorial/vllm-tutorial/03-advanced.html">
    <link rel="prev" href="02-deploy.html">
    <meta name="generator" content="Antora 3.0.0">
    <link rel="stylesheet" href="../_/css/site.css">
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://developers.redhat.com" target="_blank"><img src="../_/img/RHDLogo.svg" height="40px" alt="Red Hat Developer Program"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="https://redhat-scholars.github.io/vllm-tutorial">vLLM Tutorial</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://developers.redhat.com/ebooks/" target="_blank">Books</a>
        <a class="navbar-item" href="https://developers.redhat.com/cheatsheets/" target="_blank">Cheat Sheets</a>
        <a class="navbar-item" href="https://developers.redhat.com/events/" target="_blank">Upcoming Events</a>
        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link" href="#">More Tutorials</a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://redhat-developer-demos.github.io/kubernetes-tutorial/" target="_blank">Kubernetes</a>
            <a class="navbar-item" href="https://redhat-developer-demos.github.io/istio-tutorial/" target="_blank">Istio</a>
            <a class="navbar-item" href="https://redhat-developer-demos.github.io/quarkus-tutorial/" target="_blank">Quarkus</a>
            <a class="navbar-item" href="https://redhat-developer-demos.github.io/knative-tutorial/" target="_blank">Knative</a>
            <a class="navbar-item" href="https://redhat-developer-demos.github.io/tekton-tutorial/" target="_blank">Tekton</a>
          </div>
        </div>
      </div>
    </div>
  </nav>
</header>
<div class="body">
<div class="nav-container" data-component="vllm-tutorial" data-version="master">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="index.html"></a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="01-setup.html">1. Setup &amp; Installation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="01-setup.html#prerequisites">Prerequisites</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="01-setup.html#installation">Installation Methods</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="01-setup.html#verification">Verify Installation</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="01-setup.html#troubleshooting">Troubleshooting</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="02-deploy.html">2. Usage Examples</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="02-deploy.html#basic">Basic Usage</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="02-deploy.html#server">API Server</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="02-deploy.html#docker">Docker Deployment</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="02-deploy.html#monitoring">Monitoring</a>
  </li>
</ul>
  </li>
  <li class="nav-item is-current-page" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="03-advanced.html">3. Advanced Features</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="#parallel">Tensor Parallelism</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="#quantization">Model Quantization</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="#optimization">Performance Optimization</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="#streaming">Streaming Responses</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="#custom">Custom Models &amp; LoRA</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="#scaling">Scaling &amp; Load Balancing</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="index.html">vLLM Tutorial</a></li>
    <li><a href="03-advanced.html">3. Advanced Features</a></li>
  </ul>
</nav>
  <div class="edit-this-page"><a href="https://github.com/redhat-scholars/vllm-tutorial/edit/master/documentation/modules/ROOT/pages/03-advanced.adoc">Edit this Page</a></div>
  </div>
  <div class="content">
<article class="doc">
<h1 class="page">Advanced Features &amp; Optimization</h1>
<div class="sect1">
<h2 id="parallel"><a class="anchor" href="#parallel"></a>Tensor Parallelism</h2>
<div class="sectionbody">
<div class="paragraph">
<p>For large models that don&#8217;t fit on a single GPU, vLLM supports tensor parallelism to distribute the model across multiple GPUs.</p>
</div>
<div class="sect2">
<h3 id="_multi_gpu_setup"><a class="anchor" href="#_multi_gpu_setup"></a>Multi-GPU Setup</h3>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">from vllm import LLM, SamplingParams

# Initialize model with tensor parallelism across 4 GPUs
llm = LLM(
    model="meta-llama/Llama-2-13b-chat-hf",
    tensor_parallel_size=4,
    gpu_memory_utilization=0.9
)

sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.9,
    max_tokens=512
)

prompts = ["Explain the concept of distributed computing"]
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(output.outputs[0].text)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_server_with_tensor_parallelism"><a class="anchor" href="#_server_with_tensor_parallelism"></a>Server with Tensor Parallelism</h3>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">python -m vllm.entrypoints.openai.api_server \
    --model meta-llama/Llama-2-13b-chat-hf \
    --tensor-parallel-size 4 \
    --gpu-memory-utilization 0.9 \
    --host 0.0.0.0 \
    --port 8000</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="quantization"><a class="anchor" href="#quantization"></a>Model Quantization</h2>
<div class="sectionbody">
<div class="paragraph">
<p>vLLM supports various quantization methods to reduce memory usage and improve inference speed.</p>
</div>
<div class="sect2">
<h3 id="_awq_quantization"><a class="anchor" href="#_awq_quantization"></a>AWQ Quantization</h3>
<div class="paragraph">
<p>AWQ (Activation-aware Weight Quantization) provides significant memory savings:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">from vllm import LLM, SamplingParams

# Load AWQ quantized model
llm = LLM(
    model="TheBloke/Llama-2-7B-Chat-AWQ",
    quantization="awq",
    dtype="half"
)

sampling_params = SamplingParams(temperature=0.7, max_tokens=256)
prompts = ["What are the benefits of model quantization?"]
outputs = llm.generate(prompts, sampling_params)

print(outputs[0].outputs[0].text)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_gptq_quantization"><a class="anchor" href="#_gptq_quantization"></a>GPTQ Quantization</h3>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">from vllm import LLM, SamplingParams

# Load GPTQ quantized model
llm = LLM(
    model="TheBloke/Llama-2-7B-Chat-GPTQ",
    quantization="gptq",
    dtype="half"
)

sampling_params = SamplingParams(temperature=0.7, max_tokens=256)
prompts = ["Explain GPTQ quantization"]
outputs = llm.generate(prompts, sampling_params)

print(outputs[0].outputs[0].text)</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="optimization"><a class="anchor" href="#optimization"></a>Performance Optimization</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_memory_management"><a class="anchor" href="#_memory_management"></a>Memory Management</h3>
<div class="paragraph">
<p>Configure memory usage for optimal performance:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">from vllm import LLM, SamplingParams

# Optimize memory usage
llm = LLM(
    model="microsoft/DialoGPT-medium",
    gpu_memory_utilization=0.95,  # Use 95% of GPU memory
    max_num_batched_tokens=8192,  # Increase batch size
    max_num_seqs=256,             # Maximum sequences per batch
    max_model_len=2048            # Maximum context length
)

# Configure sampling for performance
sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    max_tokens=512,
    use_beam_search=False  # Faster than beam search
)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_batching_strategies"><a class="anchor" href="#_batching_strategies"></a>Batching Strategies</h3>
<div class="paragraph">
<p>Optimize throughput with intelligent batching:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">from vllm import LLM, SamplingParams
import asyncio

# For high-throughput scenarios
llm = LLM(
    model="microsoft/DialoGPT-medium",
    max_num_seqs=512,        # Large batch size
    max_num_batched_tokens=16384,  # High token throughput
    swap_space=4,            # Use swap space for larger batches
)

# Prepare large batch of diverse prompts
prompts = [
    "Explain artificial intelligence",
    "What is machine learning?",
    "Describe neural networks",
    "How does deep learning work?",
] * 100  # 400 total prompts

sampling_params = SamplingParams(
    temperature=0.7,
    max_tokens=128,
    stop=["\n\n"]  # Stop at double newline
)

# Process all at once for maximum throughput
outputs = llm.generate(prompts, sampling_params)
print(f"Generated {len(outputs)} responses")</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="streaming"><a class="anchor" href="#streaming"></a>Streaming Responses</h2>
<div class="sectionbody">
<div class="paragraph">
<p>For real-time applications, vLLM supports streaming token generation:</p>
</div>
<div class="sect2">
<h3 id="_basic_streaming"><a class="anchor" href="#_basic_streaming"></a>Basic Streaming</h3>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">from vllm import LLM, SamplingParams

llm = LLM(model="microsoft/DialoGPT-medium")
sampling_params = SamplingParams(
    temperature=0.7,
    max_tokens=256,
    stream=True  # Enable streaming
)

prompts = ["Write a short story about time travel"]

# Stream tokens as they're generated
for output in llm.generate(prompts, sampling_params):
    for token_output in output.outputs:
        print(token_output.text, end='', flush=True)
    print()  # New line at the end</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_streaming_with_api_server"><a class="anchor" href="#_streaming_with_api_server"></a>Streaming with API Server</h3>
<div class="paragraph">
<p>The API server supports streaming via Server-Sent Events:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">import requests
import json

def stream_chat():
    url = "http://localhost:8000/v1/chat/completions"
    data = {
        "model": "microsoft/DialoGPT-medium",
        "messages": [
            {"role": "user", "content": "Tell me about the future of AI"}
        ],
        "stream": True,
        "max_tokens": 200
    }

    with requests.post(url, json=data, stream=True) as response:
        for line in response.iter_lines():
            if line:
                line = line.decode('utf-8')
                if line.startswith('data: '):
                    data = line[6:]  # Remove 'data: ' prefix
                    if data != '[DONE]':
                        try:
                            chunk = json.loads(data)
                            content = chunk['choices'][0]['delta'].get('content', '')
                            if content:
                                print(content, end='', flush=True)
                        except json.JSONDecodeError:
                            pass
        print()  # New line at the end

stream_chat()</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="custom"><a class="anchor" href="#custom"></a>Custom Models and LoRA</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_loading_custom_models"><a class="anchor" href="#_loading_custom_models"></a>Loading Custom Models</h3>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">from vllm import LLM, SamplingParams

# Load model from local path
llm = LLM(
    model="/path/to/your/custom/model",
    trust_remote_code=True,  # For custom model architectures
    dtype="float16"
)

# Load from HuggingFace with custom config
llm = LLM(
    model="your-username/your-model-name",
    revision="main",  # Specific branch/tag
    tokenizer_mode="auto"
)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_lora_adapters"><a class="anchor" href="#_lora_adapters"></a>LoRA Adapters</h3>
<div class="paragraph">
<p>vLLM supports LoRA (Low-Rank Adaptation) for efficient fine-tuning:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest

# Load base model
llm = LLM(
    model="meta-llama/Llama-2-7b-hf",
    enable_lora=True,
    max_loras=4,  # Support up to 4 LoRA adapters
    max_lora_rank=64
)

# Define LoRA request
lora_request = LoRARequest(
    lora_name="math_adapter",
    lora_int_id=1,
    lora_local_path="/path/to/lora/adapter"
)

sampling_params = SamplingParams(temperature=0.7, max_tokens=256)
prompts = ["Solve this math problem: 2x + 5 = 15"]

# Generate with LoRA adapter
outputs = llm.generate(
    prompts,
    sampling_params,
    lora_request=lora_request
)

print(outputs[0].outputs[0].text)</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="monitoring"><a class="anchor" href="#monitoring"></a>Production Monitoring</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_prometheus_metrics"><a class="anchor" href="#_prometheus_metrics"></a>Prometheus Metrics</h3>
<div class="paragraph">
<p>vLLM exports Prometheus metrics for monitoring:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Start server with metrics enabled
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/DialoGPT-medium \
    --port 8000 \
    --enable-metrics \
    --metrics-port 8001</code></pre>
</div>
</div>
<div class="paragraph">
<p>Access metrics at <code><a href="http://localhost:8001/metrics" class="bare">http://localhost:8001/metrics</a></code>:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># View key metrics
curl <a href="http://localhost:8001/metrics" class="bare">http://localhost:8001/metrics</a> | grep -E "(request_duration|throughput|gpu_memory)"</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_health_checks_and_logging"><a class="anchor" href="#_health_checks_and_logging"></a>Health Checks and Logging</h3>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">import logging
import requests
import time

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def health_check_loop():
    while True:
        try:
            # Check API health
            response = requests.get("http://localhost:8000/health", timeout=5)
            if response.status_code == 200:
                logger.info("‚úÖ Service healthy")

                # Check model info
                models_response = requests.get("http://localhost:8000/v1/models")
                if models_response.status_code == 200:
                    models = models_response.json()
                    logger.info(f"üìä Available models: {len(models['data'])}")
            else:
                logger.error(f"‚ùå Health check failed: {response.status_code}")

        except requests.exceptions.RequestException as e:
            logger.error(f"‚ùå Connection failed: {e}")

        time.sleep(30)  # Check every 30 seconds

if __name__ == "__main__":
    health_check_loop()</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="scaling"><a class="anchor" href="#scaling"></a>Scaling and Load Balancing</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_horizontal_scaling"><a class="anchor" href="#_horizontal_scaling"></a>Horizontal Scaling</h3>
<div class="paragraph">
<p>Deploy multiple vLLM instances behind a load balancer:</p>
</div>
<div class="paragraph">
<p><strong>Docker Compose:</strong></p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># docker-compose-cluster.yml
version: '3.8'
services:
  vllm-node-1:
    image: vllm/vllm-openai:latest
    ports:
      - "8001:8000"
    command: &gt;
      --model microsoft/DialoGPT-medium
      --host 0.0.0.0
      --port 8000
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]

  vllm-node-2:
    image: vllm/vllm-openai:latest
    ports:
      - "8002:8000"
    command: &gt;
      --model microsoft/DialoGPT-medium
      --host 0.0.0.0
      --port 8000
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["1"]
              capabilities: [gpu]

  nginx:
    image: nginx:alpine
    ports:
      - "8000:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - vllm-node-1
      - vllm-node-2</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong>Podman Compose:</strong></p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># podman-compose-cluster.yml
version: '3.8'
services:
  vllm-node-1:
    image: vllm/vllm-openai:latest
    ports:
      - "8001:8000"
    command: &gt;
      --model microsoft/DialoGPT-medium
      --host 0.0.0.0
      --port 8000
    devices:
      - nvidia.com/gpu=0

  vllm-node-2:
    image: vllm/vllm-openai:latest
    ports:
      - "8002:8000"
    command: &gt;
      --model microsoft/DialoGPT-medium
      --host 0.0.0.0
      --port 8000
    devices:
      - nvidia.com/gpu=1

  nginx:
    image: nginx:alpine
    ports:
      - "8000:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - vllm-node-1
      - vllm-node-2</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_load_balancer_configuration"><a class="anchor" href="#_load_balancer_configuration"></a>Load Balancer Configuration</h3>
<div class="paragraph">
<p>Create <code>nginx.conf</code>:</p>
</div>
<div class="listingblock console-input">
<div class="content">
<pre class="highlightjs highlight"><code class="language-nginx hljs" data-lang="nginx">events {
    worker_connections 1024;
}

http {
    upstream vllm_backend {
        least_conn;
        server vllm-node-1:8000;
        server vllm-node-2:8000;
    }

    server {
        listen 80;

        location / {
            proxy_pass <a href="http://vllm_backend" class="bare">http://vllm_backend</a>;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_timeout 300s;
        }
    }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>This completes the comprehensive vLLM tutorial covering setup, usage, and advanced features!</p>
</div>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="02-deploy.html">2. Usage Examples</a></span>
</nav>
</article>
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
  </div>
</main>
</div>
<footer class="footer">
  <a class="rhd-logo" href="https://developers.redhat.com" target="_blank"></div>
</footer>
<script src="../_/js/vendor/clipboard.js"></script>
<script src="../_/js/site.js"></script>
<script async src="../_/js/vendor/highlight.js"></script>
  </body>
</html>
